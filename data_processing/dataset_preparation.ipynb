{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Prepare a subset of the LAION dataset: extract valid images and save corresponding metadata\n",
    "\n",
    "We use a subset of a single file of the LAION dataset (1.8GB):\n",
    "[12933524 rows x 8 columns]\n",
    "metadata:\n",
    "SAMPLE_ID | URL | TEXT | LICENSE | NSFW | similarity | WIDTH | HEIGHT\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import glob\n",
    "import datetime\n",
    "import shutil"
   ],
   "id": "e6eb8c5f577b0831",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ENTITY_COUNT = 500\n",
    "\n",
    "FULL_LAION_PATH = \"/Users/yavuz/data/part-00000-5b54c5d5-bbcf-484d-a2ce-0d6f73df1a36-c000.snappy.parquet\"\n",
    "PREP_DATASET_PATH = f\"/Users/yavuz/data/LAION-{ENTITY_COUNT}/\"\n",
    "\n",
    "if os.path.exists(PREP_DATASET_PATH):\n",
    "    print(f\"Warning: {PREP_DATASET_PATH} exists!\")\n",
    "else:\n",
    "    os.makedirs(PREP_DATASET_PATH)\n",
    "\n",
    "IMAGES_PATH = PREP_DATASET_PATH + \"images\"\n",
    "URLS_PATH = PREP_DATASET_PATH + \"urls.txt\"\n",
    "SUCCEEDED_URLS_PATH = PREP_DATASET_PATH + \"succeeded-urls.txt\"\n",
    "DATA_PATH = PREP_DATASET_PATH + \"metadata.parquet\""
   ],
   "id": "b16b6e93054551a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def read_safe_data(path: str, count:int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return non-nsfw entries from the full LAION dataset.\n",
    "    \"\"\"\n",
    "    print(f\"Reading {count} items from full LAION dataset...\")\n",
    "    df = pd.read_parquet(path)[:count]\n",
    "    \n",
    "    nsfw_removed_data = df[df[\"NSFW\"]==\"UNLIKELY\"]\n",
    "    print(\"Size after removing NSFW:\", len(nsfw_removed_data))\n",
    "    \n",
    "    clean_url_data = nsfw_removed_data[~nsfw_removed_data['URL'].str.contains(',')]\n",
    "    print(\"Size after removing URLs with commas:\", len(clean_url_data))\n",
    "\n",
    "    return clean_url_data"
   ],
   "id": "570a0e3ef0036051",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = read_safe_data(FULL_LAION_PATH, ENTITY_COUNT)\n",
    "data"
   ],
   "id": "5b0fb588a10035b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def write_urls(data: pd.DataFrame, path: str) -> None:\n",
    "    \"\"\"\n",
    "    Writes the URLs found in the dataframe to a file in the given path\n",
    "    \"\"\"\n",
    "    with open(path, \"w+\") as f:\n",
    "        for url in data[\"URL\"]:\n",
    "            f.write(url + \"\\n\")\n",
    "    print(f\"Finished writing {len(data)} URLs to {path}\")\n",
    "\n",
    "write_urls(data, URLS_PATH)"
   ],
   "id": "7352514bcdb86df4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def download_images(url_path: str, images_path: str):\n",
    "    \"\"\"\n",
    "    download images from text file with list of urls \n",
    "    \"\"\"\n",
    "    if os.path.exists(images_path):\n",
    "        print(f\"Warning: {images_path} exists - renaming it...!\")\n",
    "        os.rename(IMAGES_PATH, IMAGES_PATH + datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "        \n",
    "    subprocess.call([\"img2dataset\", \"--url_list=\"+url_path, \"--output_folder=\"+images_path, \"--thread_count=64\", \"--image_size=256\"])"
   ],
   "id": "e8db576ee4b7e768",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "download_images(URLS_PATH, IMAGES_PATH)",
   "id": "dce90458186e0f60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_valid_file_ids(path:str) -> list[int]:\n",
    "    \"\"\"\n",
    "    Return the ids of all files in IMAGES_PATH (recursively) that end with .jpg\n",
    "    \"\"\"\n",
    "    files = glob.glob(path+\"/*/*.jpg\")\n",
    "    files = [file.split('/')[-2:] for file in files]\n",
    "    print(f\"Found {len(files)} files\")\n",
    "    \n",
    "    ids = [int(file[1].split('.')[0]) for file in files]\n",
    "    ids.sort()\n",
    "    return ids"
   ],
   "id": "7c59cf177a9dff58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ids = get_valid_file_ids(IMAGES_PATH)\n",
    "ids"
   ],
   "id": "f5fdb8943b44da33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_with_images = data.iloc[ids]\n",
    "data_with_images"
   ],
   "id": "4ec1fa5edd3d5fd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_with_images = data_with_images.reset_index()\n",
    "data_with_images"
   ],
   "id": "ae14126ded7295ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "write_urls(data_with_images, SUCCEEDED_URLS_PATH)\n",
    "#download_images(URLS_PATH, IMAGES_PATH)"
   ],
   "id": "5733e6163b8f031c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# save metadata to parquet\n",
    "data_with_images.to_parquet(DATA_PATH)"
   ],
   "id": "cc60c7123f7af2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def move_files(images_path: str):\n",
    "    \"\"\"\n",
    "    Rename (and move files across shards) so that we have a continuous range of file names from 0 to n\n",
    "    \"\"\"\n",
    "    files = glob.glob(IMAGES_PATH+\"/*/*.jpg\")\n",
    "    files.sort()\n",
    "\n",
    "    for i in range(0, len(files)):\n",
    "        shard = str(i // 10000).zfill(5)\n",
    "        index = str(i % 10000).zfill(4)\n",
    "        \n",
    "        image_file = files[i]\n",
    "        json_file = image_file.replace(\".jpg\", \".json\")\n",
    "        \n",
    "        shutil.move(image_file, f\"{images_path}/{shard}/{shard}{index}.jpg\")\n",
    "        shutil.move(json_file, f\"{images_path}/{shard}/{shard}{index}.json\")"
   ],
   "id": "238829bdeabd7324",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "move_files(IMAGES_PATH)",
   "id": "d18a267f293b3fd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cbefb5f339d77490",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
