{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\"\"\"\n",
    "identify placeholder images by: \n",
    "- Compute pairwise similarity between a subset of image embeddings - compute similarity between a vector and the 10k vectors after it\n",
    "- Those with >0.99 cosine similarity are deemed to be placeholder images. \n",
    "- This boundary was chosen using some experimentation and visual inspection of sample images.\n",
    "\"\"\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from src.dataset_processing.find_duplicates import batch_compute_all_duplicates, batch_compute_all_duplicate_pairs, compute_set_from_duplicate_pairs, compute_all_duplicates_from_placeholder_candidiates, compute_duplicate_set_from_window"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# specify dataset and paths to work on\n",
    "DATASET_ENTITY_COUNT = 1900000 #150 #20000 #1900000\n",
    "DATASET_BASE_PATH = f\"/Users/yavuz/data/LAION-{DATASET_ENTITY_COUNT}/\"\n",
    "\n",
    "METADATA_PATH = DATASET_BASE_PATH + \"metadata.parquet\"\n",
    "IMAGES_PATH = DATASET_BASE_PATH + \"images/\"\n",
    "\n",
    "vector_path = DATASET_BASE_PATH + \"vectors/\"\n",
    "\n",
    "metadata = pd.read_parquet(METADATA_PATH)"
   ],
   "id": "f482c173210da59c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import display, Image\n",
    "def get_image(vector_id: int, images_path: str):\n",
    "    \"\"\"\n",
    "    Given a vector id and base images path (IMAGES_PATH), returns the image.\n",
    "    \"\"\"\n",
    "    shard = str(vector_id // 10000).zfill(5)\n",
    "    index = str(vector_id % 10000).zfill(4)\n",
    "    image_path = f\"{images_path}/{shard}/{shard}{index}.jpg\"\n",
    "    return Image(filename=image_path) "
   ],
   "id": "20650e90f7d4d923",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "NUM_IMAGE_EMBEDDINGS = None # None 100_000\n",
    "\n",
    "image_embeddings = np.load(vector_path + \"image_vectors.npy\")\n",
    "if NUM_IMAGE_EMBEDDINGS is None:\n",
    "    NUM_IMAGE_EMBEDDINGS = len(image_embeddings)\n",
    "image_embeddings = image_embeddings[:NUM_IMAGE_EMBEDDINGS]\n",
    "image_embeddings.shape"
   ],
   "id": "65ccec70721ceb2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Normalize embeddings and compute similarity matrix\n",
    "normalised_image_embeddings = image_embeddings / np.linalg.norm(image_embeddings, axis=1, keepdims=True)"
   ],
   "id": "5c26444452073417",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "normalised_image_embeddings.shape",
   "id": "80304405eb5e78cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# calculate placeholders using window size approach\n",
    "window_size = 10_000\n",
    "placeholder_images_path = os.path.join(vector_path, \"placeholder_images\" + str(window_size) + \"_window\")\n",
    "duplicates_for_window = compute_duplicate_set_from_window(normalised_image_embeddings, placeholder_images_path, window_size=window_size)"
   ],
   "id": "691bbd4343b46c3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(len(duplicates_for_window))",
   "id": "f9fc86e5c654d535",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# update the placeholder images by computing duplicates for the window candidates\n",
    "updated_placeholder_ids_for_window = compute_all_duplicates_from_placeholder_candidiates(duplicates_for_window, normalised_image_embeddings, threshold=0.99)\n",
    "print(len(updated_placeholder_ids_for_window))"
   ],
   "id": "28e076450951c074",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Now look at saved pleacholder ids - from 50k window size\n",
    "saved_placeholder_ids = np.load(vector_path+\"placeholder_images50000_window.npy\")\n",
    "print(len(saved_placeholder_ids))"
   ],
   "id": "c6b91704bb0b7c2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# update 50k placeholder ids by computing duplicates for the window candidates\n",
    "updated_placeholder_ids = compute_all_duplicates_from_placeholder_candidiates(saved_placeholder_ids, normalised_image_embeddings, threshold=0.99)\n",
    "print(len(updated_placeholder_ids))\n",
    "# previous: 21601 in 28 min"
   ],
   "id": "7ae80da4950396d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# identify entries in one set and not the other\n",
    "new_placeholder_ids = set(updated_placeholder_ids) - set(updated_placeholder_ids_for_window)\n",
    "print(len(new_placeholder_ids))\n",
    "\n",
    "# display 10 sample images from this difference\n",
    "SAMPLE_SIZE = 10\n",
    "for i in random.sample(list(new_placeholder_ids), SAMPLE_SIZE):\n",
    "    print(i)\n",
    "    display(get_image(i, IMAGES_PATH))"
   ],
   "id": "2eca2ce7c3b7f0fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ac5005ecd0fa9f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# identify duplicate pairs in the dataset\n",
    "duplicate_pairs = batch_compute_all_duplicate_pairs(normalised_image_embeddings, batch_size=1000)\n",
    "set_from_duplicate_pairs = compute_set_from_duplicate_pairs(duplicate_pairs)\n",
    "print(len(set_from_duplicate_pairs))"
   ],
   "id": "9d811a9b2a81bb2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(len(duplicate_pairs))\n",
    "print(len(set_from_duplicate_pairs))"
   ],
   "id": "73d450725212d45d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print sample placeholder images - how many are placeholders and which are duplicates\n",
    "SAMPLE_SIZE = 10\n",
    "# select LIMIT random samples from list(set_from_duplicate_pairs)\n",
    "for i in random.sample(list(set_from_duplicate_pairs), SAMPLE_SIZE):\n",
    "    print(i)\n",
    "    display(get_image(i, IMAGES_PATH))"
   ],
   "id": "dcca49451ca4e827",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "investigation_id = 20041 # id of the flagged non-placeholder image we want to investigate",
   "id": "59330b75adc69f5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# find duplicate ids for the investigation id\n",
    "def find_duplicate_ids(id, duplicate_pairs):\n",
    "    duplicate_ids = []\n",
    "    for pair in duplicate_pairs:\n",
    "        if id == pair[0]:\n",
    "            duplicate_ids.append(pair[1])\n",
    "        elif id == pair[1]:\n",
    "            duplicate_ids.append(pair[0])\n",
    "    return duplicate_ids"
   ],
   "id": "7c1905d3c6ffb300",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# now print the text, url, and image for all duplicates of this investigated id\n",
    "LIMIT = 20\n",
    "duplicate_ids = find_duplicate_ids(investigation_id, duplicate_pairs)\n",
    "\n",
    "if len(duplicate_ids) > LIMIT:\n",
    "    print(f\"Found {len(duplicate_ids)} duplicates, truncating to {LIMIT}\")\n",
    "    duplicate_ids = duplicate_ids[:LIMIT]"
   ],
   "id": "2cf201faaa7fe835",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(investigation_id, metadata[\"TEXT\"].iloc[investigation_id], metadata[\"URL\"].iloc[investigation_id])\n",
    "display(get_image(investigation_id, IMAGES_PATH))\n",
    "for duplicate_id in duplicate_ids:\n",
    "    print(duplicate_id, metadata[\"TEXT\"].iloc[duplicate_id], metadata[\"URL\"].iloc[duplicate_id])\n",
    "    display(get_image(duplicate_id, IMAGES_PATH))"
   ],
   "id": "b2055dd54decd8b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "913f70e1ce2f0ce2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# MANUAL COMPUTATION WITH CHECKPOINTING\n",
    "near_duplicates = set()\n",
    "threshold = 0.99\n",
    "window_size = 100000\n",
    "for i in tqdm(range(0, len(normalised_image_embeddings))):\n",
    "    start = i + 1\n",
    "    end = min(i + window_size, len(normalised_image_embeddings))\n",
    "    if start < end:\n",
    "        scores = np.dot(normalised_image_embeddings[i], normalised_image_embeddings[start:end].T)\n",
    "\n",
    "        near_duplicate_indices = np.where(scores > threshold)[0] + start\n",
    "        if len(near_duplicate_indices) > 0:\n",
    "            near_duplicates.update([i] + list(near_duplicate_indices))\n",
    "\n",
    "    # Save checkpoint every 50k iterations\n",
    "    if i % 50000 == 0:\n",
    "        np.save(vector_path+\"placeholder_images\"+str(window_size)+\"Window_checkpoint\", np.array(list(near_duplicates)))\n",
    "len(near_duplicates)"
   ],
   "id": "e853eff40836b007",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np.save(vector_path+\"placeholder_images\"+str(window_size)+\"Window\", np.array(list(near_duplicates)))",
   "id": "9511dd31dc77998a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3bb4d71f872da35f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# there are 164 images identified in 20k dataset\n",
    "# there are 5-10 images which are not placeholder, but are identified because exact duplicates exist in the dataset\n",
    "placeholder_images = np.load(vector_path+\"placeholder_images.npy\")\n",
    "placeholder_images.shape"
   ],
   "id": "83111b38a3051cc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "placeholder_images",
   "id": "d5f24ee71abefd51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "838538e125b11dbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print sample placeholder images\n",
    "LIMIT = 10\n",
    "for i in placeholder_images[len(placeholder_images) - LIMIT:]:\n",
    "    print(i)\n",
    "    display(get_image(i, IMAGES_PATH))"
   ],
   "id": "57b6d86096660c90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7c0c3727481b5ed6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
